#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
train_student_kd.py

–î–æ–ø–æ–ª–Ω—è–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π pipeline:
 ‚Ä¢¬†–≤–≤–æ–¥–∏—Ç –∫–∞—Å—Ç–æ–º‚Äë–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é,
 ‚Ä¢¬†–æ–±—É—á–∞–µ—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞ –Ω–∞ 64√ó64 (latents¬†8√ó8),
 ‚Ä¢¬†–∫–∞–∂–¥—ã–µ¬†500 —à–∞–≥–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç sample‚Äë–ø–∞—Ä—ã teacher/student –∏ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ —Å—á–∏—Ç–∞–µ—Ç FID,
 ‚Ä¢¬†–≤ –∫–æ–Ω—Ü–µ —ç–ø–æ—Ö–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç 3¬†–ø—Ä–∏–º–µ—Ä–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç—É–¥–µ–Ω—Ç–∞,
 ‚Ä¢¬†–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–≤—É—Ö —Ä–µ–∂–∏–º–æ–≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ (random|custom) –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.

–†–∞—Å—á—ë—Ç FID/LPIPS ‚Äï torchmetrics (GPU) –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
—ç–∫—Å—Ç—Ä–∞‚Äë–≥—Ä–∞—Ñ–∏–∫–æ–≤, —á—Ç–æ–±—ã –ø–æ–º–µ—Å—Ç–∏—Ç—å—Å—è –Ω–∞¬†RTX¬†3060¬†6‚ÄØGB.
"""
from prepare_unet_small import slim_sd_small

import os, math, time, shutil, gc, random, argparse
from pathlib import Path
from functools import partial
import prepare_unet_small
import torch, torch.nn.functional as F
from torch import nn
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.utils import save_image
from transformers import CLIPFeatureExtractor

from diffusers import (
    StableDiffusionPipeline, UNet2DConditionModel,
    AutoencoderKL, DDPMScheduler,
)
from prepare_unet_small import prepare_unet
from transformers import CLIPTextModel, CLIPTokenizer
from torchmetrics.image.fid import FrechetInceptionDistance
from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity as LPIPS

from accelerate import Accelerator
from accelerate.logging import get_logger
from accelerate.utils import ProjectConfiguration
from coco_dataset import get_coco_dataloader

# ------------------------------------------------------------
# 0.  –ö–∞—Å—Ç–æ–º–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
# ------------------------------------------------------------
def parse_args():
    import argparse, os

    parser = argparse.ArgumentParser(description="Distill student SD model")
    parser.add_argument(
        "--pretrained_model_name_or_path",
        type=str,
        required=True,
        help="HuggingFace ID –∏–ª–∏ –ø—É—Ç—å –∫ —É—á–∏—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ–ª–∏",
    )
    parser.add_argument(
        "--revision",
        type=str,
        default=None,
        help="–í–µ—Ç–∫–∞/—Ç–µ–≥/–∫–æ–º–º–∏—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏ feature_extractor",
    )
    parser.add_argument(
        "--train_data_dir",
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ COCO (train2017)",
    )
    parser.add_argument(
        "--caption_file",
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É captions_train2017.json",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="distill_runs",
        help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã –∏ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å",
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=3,
        help="–ß–∏—Å–ª–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è",
    )
    parser.add_argument(
        "--max_train_samples",
        type=int,
        default=None,
        help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å —á–∏—Å–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ COCO (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)",
    )
    parser.add_argument(
        "--distill_level",
        type=str,
        default="sd_small",
        choices=["sd_small", "sd_tiny"],
        help="–£—Ä–æ–≤–µ–Ω—å ¬´—Å–∂–∞—Ç–∏—è¬ª —Å—Ç—É–¥–µ–Ω—Ç–∞",
    )
    parser.add_argument(
        "--max_train_steps",
        type=int,
        default=None,
        help="–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –æ–±—â–µ–µ —á–∏—Å–ª–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤",
    )

    return parser.parse_args()


def initialize_student_weights(teacher_unet_full: UNet2DConditionModel,
                               student_unet: UNet2DConditionModel):
    """
    –ü—Ä–æ–µ–∫—Ü–∏—è/–æ–±—Ä–µ–∑–∫–∞ –≤–µ—Å–æ–≤ TEACHER‚ÄëUNet ‚Üí STUDENT‚ÄëUNet.
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–≤–ø–∞–¥–∞—é—â–∏–µ —Ç–µ–Ω–∑–æ—Ä—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é,
    –∞ –±–æ–ª–µ–µ ¬´—Ç–æ–ª—Å—Ç—ã–µ¬ª ‚Äë‚Äë —Å—Ä–µ–∑–∞–µ—Ç –ø–æ¬†–∫–∞–Ω–∞–ª–∞–º.
    """
    teacher_state = teacher_unet_full.state_dict()
    student_state = student_unet.state_dict()
    new_state = {}

    for name, t_w in teacher_state.items():

        if name.startswith("down_blocks.3.") or name.startswith("up_blocks.0."):
            continue          # —ç—Ç–∏ –±–ª–æ–∫–∏ —É —Å—Ç—É–¥–µ–Ω—Ç–∞ –≤—ã–±—Ä–æ—à–µ–Ω—ã

        # --- –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ up‚Äë–±–ª–æ–∫–æ–≤ (1‚Üí0, 2‚Üí1, 3‚Üí2)
        if name.startswith("up_blocks"):
            parts = name.split(".")
            idx = int(parts[1])
            if idx == 0:
                continue
            parts[1] = str(idx - 1)
            s_name = ".".join(parts)

        # --- down‚Äëblocks (0,1,2)¬†‚Äî¬†–∏–Ω–¥–µ–∫—Å —Å–æ–≤–ø–∞–¥–∞–µ—Ç
        elif name.startswith("down_blocks"):
            if int(name.split(".")[1]) >= 3:
                continue
            s_name = name

        else:                 # mid, conv_in/out, time_embed
            s_name = name

        if s_name not in student_state:
            continue

        s_w = student_state[s_name]
        if s_w.shape == t_w.shape:
            new_state[s_name] = t_w.clone()
        else:
            if t_w.ndim == 4:           # Conv [out, in, k, k]
                new_state[s_name] = t_w[: s_w.shape[0], : s_w.shape[1]].clone()
            elif t_w.ndim == 2:         # Linear
                new_state[s_name] = t_w[: s_w.shape[0], : s_w.shape[1]].clone()
            else:                       # Bias / Norm
                new_state[s_name] = t_w[: s_w.shape[0]].clone()

    student_unet.load_state_dict(new_state, strict=False)
    return student_unet


# ------------------------------------------------------------
# 1.  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ FID‚Äë/LPIPS‚Äë–º–µ—Ç—Ä–∏–∫–∏
# ------------------------------------------------------------
@torch.no_grad()
def _generate(pipeline: StableDiffusionPipeline,
              prompt: str,
              num_images: int,
              generator: torch.Generator):
    imgs = pipeline(
        prompt,
        negative_prompt=None,
        num_inference_steps=20,
        guidance_scale=7.,
        num_images_per_prompt=num_images,
        height=512, width=512,
        generator=generator,
    ).images
    return imgs


@torch.no_grad()
def compute_metrics(student_pipe, teacher_pipe, prompts, n_img=8, device="cuda"):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (FID, LPIPS)."""
    fid = FrechetInceptionDistance(feature=2048, normalize=True).to(device)
    lpips = LPIPS(net_type='alex').to(device)

    for p in prompts:
        gen = torch.Generator(device).manual_seed(42)
        st_imgs = _generate(student_pipe, p, n_img, gen)
        te_imgs = _generate(teacher_pipe, p, n_img, gen)

        st_t = torch.stack([transforms.ToTensor()(i) for i in st_imgs]).to(device)
        te_t = torch.stack([transforms.ToTensor()(i) for i in te_imgs]).to(device)

        fid.update(st_t, real=False)
        fid.update(te_t, real=True)

        lpips.update(st_t, te_t)

    return fid.compute().item(), lpips.compute().item()


# ------------------------------------------------------------
# 2.  –û—Å–Ω–æ–≤–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
# ------------------------------------------------------------
def train_student(args, init_mode="custom"):
    accelerator = Accelerator(
        project_config=ProjectConfiguration(project_dir=args.output_dir),
        mixed_precision="fp16",
        gradient_accumulation_steps=1,
    )

    # ---------- –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã teacher ----------
    teacher_pipe = StableDiffusionPipeline.from_pretrained(
        args.pretrained_model_name_or_path,
        revision=None,
        torch_dtype=torch.float16,
        safety_checker=None,
    )
    teacher_pipe.to(accelerator.device)
    teacher_unet = teacher_pipe.unet.eval().requires_grad_(False)

    # ---------- –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã student ----------
    student_unet = UNet2DConditionModel.from_pretrained(
        args.pretrained_model_name_or_path, subfolder="unet"
    )

    prepare_unet(student_unet, model_type=args.distill_level)

    if init_mode == "custom":
        initialize_student_weights(teacher_unet, student_unet)

    # --- –Ω–µ–∏–∑–º–µ–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏ ---
    tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder="tokenizer")
    text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder="vae")
    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")

    # --- –¥–∞—Ç–∞—Å–µ—Ç COCO ----
    from coco_dataset import get_coco_dataloader   # tiny helper ‚ûú –≤–µ—Ä–Ω—ë—Ç DataLoader
    train_loader = get_coco_dataloader(args, tokenizer)

    # --- –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä ---
    opt = torch.optim.AdamW(student_unet.parameters(), lr=1e-4)

    # --- —É—Å–∫–æ—Ä–µ–Ω–∏—è ---
    student_unet, opt, train_loader = accelerator.prepare(
        student_unet, opt, train_loader
    )
    # –ï—Å–ª–∏ –º—ã –≤ fp16 ‚Äî –ø—Ä–∏–≤–æ–¥–∏–º VAE –∏ —Ç–µ–∫—Å—Ç-—ç–Ω–∫–æ–¥–µ—Ä –∫ half, —á—Ç–æ–±—ã —Å–æ–≤–ø–∞–¥–∞–ª —Ç–∏–ø –≤—Ö–æ–¥–∞ –∏ bias
    if accelerator.mixed_precision == "fp16":
        vae.to(accelerator.device)
        vae.half()
        text_encoder.to(accelerator.device)
        text_encoder.half()

    scaler = GradScaler(enabled=accelerator.mixed_precision == "fp16")

    # --- –¥–ª—è –±—ã—Å—Ç—Ä–æ–π FID –æ—Ü–µ–Ω–∫–∏ ---
    fid_prompts = ["a photo of a cat", "a city skyline at sunset"]

    # --------------------------------------------------------
    from tqdm.auto import tqdm
    import torch.nn.functional as F
    feature_extractor = CLIPFeatureExtractor.from_pretrained(
        args.pretrained_model_name_or_path,
        subfolder="feature_extractor",
        revision=args.revision,
    )
    # –≤–Ω—É—Ç—Ä–∏ –≤–∞—à–µ–π —Ñ—É–Ω–∫—Ü–∏–∏ train_student, –ø–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –º–æ–¥–µ–ª–µ–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞:
    global_step = 0
    for epoch in range(args.epochs):
        student_unet.train()
        pbar = tqdm(train_loader, desc=f"[{init_mode}] Epoch {epoch}", disable=not accelerator.is_local_main_process)
        for batch in pbar:
            with accelerator.accumulate(student_unet):
                pixel_values = batch["pixel_values"].to(accelerator.device, dtype=torch.float16)
                input_ids = batch["input_ids"].to(accelerator.device)

                latents = vae.encode(pixel_values).latent_dist.sample() * vae.config.scaling_factor

                noise = torch.randn_like(latents)
                timesteps = torch.randint(
                    0,
                    noise_scheduler.config.num_train_timesteps,
                    (latents.shape[0],),
                    device=latents.device,
                ).long()

                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

                encoder_hidden_states = text_encoder(input_ids)[0]

                with torch.no_grad():
                    teacher_pred = teacher_unet(noisy_latents, timesteps, encoder_hidden_states).sample
                student_pred = student_unet(noisy_latents, timesteps, encoder_hidden_states).sample

                loss_kd = F.mse_loss(student_pred.float(), teacher_pred.float())
                loss_task = F.mse_loss(student_pred.float(), noise.float())
                loss = 0.5 * loss_task + 0.5 * loss_kd

                accelerator.backward(loss)
                opt.step()
                opt.zero_grad()

            if accelerator.sync_gradients:
                global_step += 1
                if args.max_train_steps and global_step >= args.max_train_steps:
                    done = True
                    break

                pbar.update(1)
                pbar.set_postfix({"loss": loss.item(), "step": global_step})


                if global_step % 500 == 0 and accelerator.is_main_process:
                    student_pipe = StableDiffusionPipeline(
                        vae=vae, text_encoder=text_encoder,
                        tokenizer=tokenizer, unet=accelerator.unwrap_model(student_unet),
                        scheduler=teacher_pipe.scheduler,
                        safety_checker=None, feature_extractor = feature_extractor,

                    ).to(accelerator.device, dtype=torch.float16)

                    fid, lp = compute_metrics(student_pipe, teacher_pipe,
                                              fid_prompts, n_img=4, device=accelerator.device)
                    accelerator.print(f"[{init_mode}] step {global_step}: FID={fid:.2f}, LPIPS={lp:.3f}")

                    # —ç–∫–æ–Ω–æ–º–∏–º –ø–∞–º—è—Ç—å
                    del student_pipe
                    torch.cuda.empty_cache()

                # ---------- save state ----------
                if global_step % 1000 == 0 and accelerator.is_main_process:
                    save_dir = Path(args.output_dir, f"{init_mode}_ckpt_{global_step}")
                    accelerator.save_state(save_dir)

        if done: break

        # ---------- 3 sample images –≤ –∫–æ–Ω—Ü–µ —ç–ø–æ—Ö–∏ ----------
        if accelerator.is_main_process:
            student_pipe = StableDiffusionPipeline(
                vae=vae, text_encoder=text_encoder,
                tokenizer=tokenizer, unet=accelerator.unwrap_model(student_unet),
                scheduler=teacher_pipe.scheduler, safety_checker=None,
                feature_extractor=feature_extractor,
            ).to(accelerator.device, dtype=torch.float16)

            rng = torch.Generator(accelerator.device).manual_seed(1234)
            out_imgs = _generate(student_pipe, "highly detailed photo", 3, rng)
            for i, im in enumerate(out_imgs):
                im.save(Path(args.output_dir, f"{init_mode}_epoch{epoch}_{i}.png"))

            del student_pipe
            torch.cuda.empty_cache()

    # --- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π pipeline –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è ---
    return accelerator.unwrap_model(student_unet)


# ------------------------------------------------------------
# 3.  –ó–∞–ø—É—Å–∫ –¥–≤—É—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–π –∏ –∏—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
# ------------------------------------------------------------
def evaluate_inits(args):
    log = get_logger("distill")
    accelerator = Accelerator(
        project_config=ProjectConfiguration(project_dir=args.output_dir),
        mixed_precision="fp16",
        gradient_accumulation_steps=1,
    )

    # ------------ RANDOM -------------
    log.info(">> RANDOM init")
    rnd_student = train_student(args, init_mode="random")
    torch.cuda.empty_cache() ; gc.collect()

    # ------------ CUSTOM -------------
    log.info(">> CUSTOM init")
    custom_student = train_student(args, init_mode="custom")
    torch.cuda.empty_cache() ; gc.collect()

    # ------------ FID —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ -------------
    teacher_pipe = StableDiffusionPipeline.from_pretrained(
        args.pretrained_model_name_or_path, torch_dtype=torch.float16,
        safety_checker=None).to(accelerator.device)

    tokenizer = teacher_pipe.tokenizer
    text_encoder = teacher_pipe.text_encoder
    vae = teacher_pipe.vae

    def make_pipe(unet):
        return StableDiffusionPipeline(
            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,
            unet=unet, scheduler=teacher_pipe.scheduler,
            safety_checker=None).to(accelerator.device, dtype=torch.float16)

    rnd_pipe = make_pipe(rnd_student)
    cus_pipe = make_pipe(custom_student)

    prompts = ["a photo of a dog", "an ancient temple in the jungle"]
    fid_rnd, _  = compute_metrics(rnd_pipe, teacher_pipe, prompts, n_img=8, device=accelerator.device)
    fid_cus, _  = compute_metrics(cus_pipe, teacher_pipe, prompts, n_img=8, device=accelerator.device)

    log.info(f"FINAL  FID random={fid_rnd:.2f}   FID custom={fid_cus:.2f}")
    if fid_cus < fid_rnd:
        log.info("üéâ  Custom init –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å–ª—É—á–∞–π–Ω—É—é!")

    # --- —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≥–æ—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å ---
    if accelerator.is_main_process:
        Path(args.output_dir, "final").mkdir(parents=True, exist_ok=True)
        cus_pipe.save_pretrained(Path(args.output_dir, "final"))


# ------------------------------------------------------------
# 4.  CLI
# ------------------------------------------------------------
def main():
    args = parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    evaluate_inits(args)



if __name__ == "__main__":
    main()
